#+TITLE: 確率的勾配降下法の実装

\[
\theta_j=\theta_j-\eta(f_\theta(x^{(k)})-y^{(k)})x_j^{(k)}
\]

#+BEGIN_SRC jupyter-python :session py :dir .
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

# 学習データを読み込む
train = np.loadtxt('click.csv', delimiter=',', skiprows=1)
train_x = train[:,0]
train_y = train[:,1]

# 標準化
mu = train_x.mean()
sigma = train_x.std()
def standardize(x):
    return (x - mu) / sigma

train_z = standardize(train_x)

# 学習データの行列を作る
def to_matrix(x):
    return np.vstack([np.ones(x.shape[0]), x, x ** 2]).T

X = to_matrix(train_z)

# 予測関数
def f(x):
    return np.dot(x, theta)

def mse(x, y):
    return (1 / x.shape[0]) * np.sum((y - f(x)) ** 2)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter-python :session py
# パラメーターを初期化
theta = np.random.rand(3)

# 平均二乗誤差の履歴
errors = []

# 学習率
eta = 1e-3

# 誤差の差分
diff = 1

# 学習を繰り返す
errors.append(mse(X, train_y))
while diff > 1e-2:
    # 学習データを並び替えるためにランダムな配列を用意する
    p = np.random.permutation(X.shape[0])
    # 学習データをランダムに取り出して確率的勾配降下法でパラメータ更新
    for x, y in zip(X[p,:], train_y[p]):
        theta = theta - eta * (f(x) - y) * x
        # 前回の誤差との差分を計算
    errors.append(mse(X, train_y))
    diff = errors[-2] - errors[-1]
#+END_SRC

#+RESULTS:

#+begin_src jupyter-python :session py
x = np.linspace(-3, 3, 100)

plt.plot(train_z, train_y, 'o')
plt.plot(x, f(to_matrix(x)))
plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/aac4832619285372c1602fcf7f984bd8003048ca.png]]

#+begin_src jupyter-python :session py
# 誤差をプロット
x = np.arange(len(errors))

plt.plot(x, errors)
plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/8f980bc64ffb2d181618ac40b5f76c9e1ec914b2.png]]
