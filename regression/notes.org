* Regression

** Bias-Variance Decomposition

\[
E[(y_0-f_{est}(x_0))^2]=(f_{true}(x_0)-E[f_{est}(x_0)])^2+Var[f_{est}(x_0)]+Var[\epsilon]
\]

** Regularization

- 共線性 :: 特徴量間の相関が強すぎる
- 共線性が疑われる変数の1つを外す
- ペナルティ項を足す
  - ペナルティ項 :: $$\lambda \sum_{k=1}^d|\beta_k|^p$$
  - $p=1$ なら *Lasso 回帰* (L1 正則化)
  - $p=2$ なら *Ridge 回帰* (L2 正則化)

** Regression Spline Method

\[
h_1(X)=1,h_2(X)=X,h_3(X)=X^2,h_4=X^3,h_5(X)=(X-\xi_1)^3_+,h_6(X)=(X-\xi_2)^3_+
\]

\[
y~\sum_{i=1}^6\beta_i h_i(x)
\]

- 基底 $h_i(X)$ の数 :: 領域数 * (次数 + 1) - 次数 * 境界数

** Gradient Boosting

比較的浅い回帰木を繰り返し学習することで少しずつ推定精度を上げていく。

$r=y$

$r$ に対して $d$ 分割の回帰木をフィットする。結果を $f^1(x)$ とする。

$r \leftarrow r-\lambda f^1(x)$

$r$ に対して $d$ 分割の回帰木をフィットする。結果を $f^2(x)$ とする。

$r \leftarrow r-\lambda f^2(x)$
