* Optimization

** Objective Function

\[
E(\theta)=\frac12\sum_{i=1}^n(y^{(i)}-f_\theta(x^{(i)}))^2
\]

** Gradient Descennt, Steepest Descent

\[
x:=x-\eta\frac{d}{dx}g(x)
\]

** Stochastic Gradient Descent

\[
\theta_j:=\theta_j-\eta\sum_{k\in K}(f_\theta(x^{(k)})-y^{(k)})x_j^{(k)}
\]

- $K$ :: ランダムに選ばれた学習データのインデックスの集合

** Method of Lagrange Multiplier

\[
\max_\theta\sum_{i=1}^3n_i\log\theta_i
\]

subject to

\[
\sum_{i=1}^3\theta_i=1
\]

*** 制約式の部分をペナルティとして元の式に含む方法

\[
L_1=\max_\theta\sum_{i=1}^3n+i\log\theta_i-\lambda(\sum_{i=1}^3\theta_i-1)^2
\]

*** 制約を二乗せずにそのまま含める -> ラグランジュ乗数法

\[
L_2=\max_\theta\sum_{i=1}^3n_i\log\theta_i-\lambda(\sum_{i=1}^3\theta_i-1)
\]
